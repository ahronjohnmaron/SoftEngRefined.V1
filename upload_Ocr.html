<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OCR Autofill w/ Groq Demo</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/tesseract.js@5/dist/tesseract.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cropperjs/1.5.13/cropper.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cropperjs/1.5.13/cropper.min.js"></script>
    <style>
        body { font-family: 'Poppins', sans-serif; }
    </style>
</head>
<body class="bg-gray-900 text-white min-h-screen flex flex-col items-center justify-center p-4">
  <div class="max-w-xl w-full space-y-6">
    <h1 class="text-2xl font-bold text-teal-400 mb-2">OCR Autofill - Groq Structured Demo</h1>
    <div class="bg-gray-800 rounded-lg p-4">
      <label for="imageInput" class="block font-semibold mb-2">Upload Image:</label>
      <input type="file" id="imageInput" accept="image/*" class="mb-4 w-full text-sm rounded border border-gray-400 px-2 py-1 bg-gray-900 text-white">
      <div class="mb-2">
        <img id="cropImage" class="rounded border border-gray-600 w-full max-w-lg max-h-[400px] mx-auto hidden" />
      </div>
      <div class="flex gap-2">
        <button id="cropBtn" class="bg-indigo-400 hover:bg-indigo-300 px-3 py-2 rounded font-semibold disabled:bg-gray-600" disabled>Crop & Use This Image</button>
        <button id="extractText" class="bg-teal-500 hover:bg-teal-400 px-3 py-2 rounded font-bold disabled:bg-gray-500" disabled>Extract Text from Image</button>
      </div>
      <div id="progressContainer" class="mt-4 hidden">
        <div class="bg-gray-700 h-2 rounded-full overflow-hidden mb-2">
          <div class="bg-teal-400 h-2 w-0 transition-all" id="progressBar"></div>
        </div>
        <div class="text-teal-300 font-semibold text-sm text-center" id="progressText" aria-live="polite">Starting OCR...</div>
      </div>
    </div>
    <div class="bg-gray-800 rounded-lg p-4">
      <div class="text-teal-300 font-bold mb-2">Extracted Text Preview:</div>
      <pre id="extractedTextPreview" class="bg-gray-950 text-gray-400 rounded p-2 min-h-[48px] break-all">No text extracted yet</pre>
    </div>
    <form class="bg-gray-800 rounded-lg p-4 space-y-4">
      <div>
        <label class="block mb-1 font-semibold" for="fieldWhen">When (date/time):</label>
        <input id="fieldWhen" class="w-full bg-gray-900 border border-gray-600 rounded px-3 py-2 focus:border-teal-500" autocomplete="off" />
      </div>
      <div>
        <label class="block mb-1 font-semibold" for="fieldDatePosted">Date posted:</label>
        <input id="fieldDatePosted" class="w-full bg-gray-900 border border-gray-600 rounded px-3 py-2 focus:border-teal-500" autocomplete="off" />
      </div>
      <div>
        <label class="block mb-1 font-semibold" for="fieldWhere">Where (location):</label>
        <input id="fieldWhere" class="w-full bg-gray-900 border border-gray-600 rounded px-3 py-2 focus:border-teal-500" autocomplete="off" />
      </div>
      <div>
        <label class="block mb-1 font-semibold" for="fieldWhat">What (event/content):</label>
        <textarea id="fieldWhat" class="w-full bg-gray-900 border border-gray-600 rounded px-3 py-2 focus:border-teal-500 min-h-[60px]"></textarea>
      </div>
    </form>
    <div id="statusMsg" class="text-yellow-400 text-sm mt-2"></div>
  </div>

  <script>
    let cropper = null;
    let currentImageBlob = null;
    const imageInput = document.getElementById('imageInput');
    const cropImage = document.getElementById('cropImage');
    const cropBtn = document.getElementById('cropBtn');
    const extractTextBtn = document.getElementById('extractText');
    const extractedTextPreview = document.getElementById('extractedTextPreview');
    const progressContainer = document.getElementById('progressContainer');
    const progressBar = document.getElementById('progressBar');
    const progressText = document.getElementById('progressText');
    const fieldWhen = document.getElementById('fieldWhen');
    const fieldDatePosted = document.getElementById('fieldDatePosted');
    const fieldWhere = document.getElementById('fieldWhere');
    const fieldWhat = document.getElementById('fieldWhat');
    const statusMsg = document.getElementById('statusMsg');

    imageInput.addEventListener('change', (e) => {
      const file = e.target.files[0];
      if (file) {
        const reader = new FileReader();
        reader.onload = (e) => {
          cropImage.src = e.target.result;
          cropImage.classList.remove("hidden");
          if (cropper) cropper.destroy();
          cropper = new Cropper(cropImage, {
            viewMode: 1,
            autoCropArea: 1,
            movable: true,
            zoomable: true,
            rotatable: true,
            scalable: true,
            background: false,
            minCropBoxWidth: 100,
            minCropBoxHeight: 100,
          });
          cropBtn.disabled = false;
          extractTextBtn.disabled = true;
        };
        reader.readAsDataURL(file);
      }
    });

    cropBtn.addEventListener('click', () => {
      if (cropper) {
        cropper.getCroppedCanvas().toBlob(blob => {
          currentImageBlob = blob;
          statusMsg.textContent = "Image cropped. Now you can run OCR.";
          extractTextBtn.disabled = false;
        });
      }
    });

    extractTextBtn.addEventListener('click', async () => {
      if (!currentImageBlob) {
        statusMsg.textContent = "Please crop and confirm the image to use for OCR.";
        return;
      }
      statusMsg.textContent = '';
      progressBar.style.width = "0%";
      progressContainer.classList.remove('hidden');
      extractTextBtn.disabled = true;
      extractTextBtn.textContent = 'Recognizing...';
      try {
        const result = await Tesseract.recognize(currentImageBlob, 'eng', {
          logger: m => {
            if (m.status === 'recognizing text') {
              const progress = Math.round(m.progress * 100);
              progressBar.style.width = progress + '%';
              progressText.textContent = `OCR ${progress}%`;
            }
          }
        });
        const extractedText = result.data.text.trim();
        extractedTextPreview.textContent = extractedText || "No text found";
        statusMsg.textContent = "Asking Groq for structured details...";

        // Call Groq for structured fields:
        const fields = await extractSmartFieldsWithGroq(extractedText);
        statusMsg.textContent = fields ? "Fields filled by Groq AI!" : "Groq was unable to extract fields.";
        
        fieldWhen.value = fields.when || "";
        fieldDatePosted.value = fields.date_posted || "";
        fieldWhere.value = fields.where || "";
        fieldWhat.value = fields.what || "";

      } catch (err) {
        statusMsg.textContent = "ERROR: " + err;
      } finally {
        extractTextBtn.disabled = false;
        extractTextBtn.textContent = 'Extract Text from Image';
        progressContainer.classList.add('hidden');
      }
    });

    // --- GROQ STRUCTURED OUTPUT FUNCTION ---
    async function extractSmartFieldsWithGroq(ocrText) {
      const api_key = "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"; // <-- Insert your valid Groq API key here!
      const prompt = `
Given this text:
"""${ocrText}"""
Return ONLY a JSON object with these keys if present:
- when (the date/time of the event)
- where (the full location/venue or institution)
- what (a complete, clear, self-contained summary of the main event or announcement targetâ€”at least 2 sentences; if possible, include the request or the core information/decision).
- date_posted (the actual posting date if found).

Respond in JSON only, no explanation.
`;

      const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${api_key}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "llama-3.3-70b-versatile",
          messages: [
            { role: "system", content: "You extract only JSON." },
            { role: "user", content: prompt }
          ],
          max_tokens: 256,
          temperature: 0.1,
        }),
      });
      const data = await response.json();
      try {
        const match = data.choices[0].message.content.match(/\{[\s\S]*\}/);
        if (match) return JSON.parse(match[0]);
        return {};
      } catch {
        return {};
      }
    }
  </script>
</body>
</html>
